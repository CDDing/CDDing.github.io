---
title: GPU Programming Introduction (GPU 프로그래밍 입문) — 한글 정리
description: Georgia Tech CS8803 Module 3 — CUDA 프로그래밍 기초, 벡터 덧셈, 블록/스레드 계층, 공유 메모리, 점유율, 스텐실 연산
---

> **원문:** [CS8803 OMSCS — GPU Hardware and Software Notes (Module 3)](https://lowyx.com/posts/gt-gpu-notes/#module-3-gpu-programming-introduction)

## 학습 목표

- GPU 프로그래밍의 기초를 설명한다
- CUDA를 사용하여 프로그래밍할 수 있다

**필수 자료:** [CUDA Programming Guide, Section 3](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)

---

## Lesson 1: CUDA 프로그래밍 소개

### CUDA 코드 예제: 벡터 덧셈

**커널 코드 (GPU에서 실행):**

```cuda
__global__ void vectorAdd(const float *A, const float *B,
                          float *C, int numElements) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < numElements) {
        C[i] = A[i] + B[i] + 0.0f;
    }
}
```

**호스트 코드 (CPU에서 실행):**

```cuda
int main(void) {
    // 디바이스 입력 벡터 A 할당
    float *d_A = NULL;
    err = cudaMalloc((void **)&d_A, size);

    // 호스트 메모리의 입력 벡터를 디바이스 메모리로 복사
    err = cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);

    // 커널 실행
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);

    // 디바이스 결과를 호스트 메모리로 복사
    err = cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
}
```

호스트 코드는 CPU에서 실행되며, 데이터 읽기와 메모리 관리를 담당한다. 삼중 꺾쇠(`<<<>>>`)로 커널을 호출하면서 설정 파라미터(그리드당 블록 수, 블록당 스레드 수)와 커널 인자를 전달한다. 커널은 GPU에서 병렬 처리된다.

### SPMD와 threadIdx.x

GPU는 SPMD(Single Program, Multiple Data) 패러다임에 따라 코드를 동시에 실행한다. 예를 들어 4개 스레드가 병렬로 각각 "hello, I'm an OMSCS student"를 출력하면, 출력 순서는 보장되지 않는다. 그러나 각 스레드는 서로 다른 데이터에 대해 동작해야 한다. 각 스레드에 고유 식별자를 부여하기 위해 내장 변수 `threadIdx.x`를 사용한다.

```cuda
vectorAdd(/* 인자 */) {
    int idx = threadIdx.x;
    c[idx] = a[idx] + b[idx];
}
```

각 스레드는 스레드 ID로 결정되는 `a[idx]`와 `b[idx]`에 접근하여 서로 다른 벡터 요소에 대해 동작한다.

### 실행 계층 구조

CUDA는 워프를 넘어서는 계층적 실행 구조를 도입한다. 스레드가 **블록(block)** 으로 그룹화되고, 블록들이 동시에 실행된다.

- 워프는 프로그래머가 처음에 이해하지 않아도 되는 마이크로 아키텍처 개념이다.
- **블록은 CUDA 프로그래밍에서 핵심 개념**이다.
- 각 블록은 하나의 SM에서 실행된다.
- CUDA 블록 간 실행 순서는 보장되지 않는다.
- 블록마다 메모리 접근 범위가 다르므로 데이터를 블록 간에 분할한다.

### 데이터 인덱싱

**내장 변수:**
- `threadIdx.x`, `threadIdx.y`, `threadIdx.z` — 스레드 인덱스
- `blockIdx.x`, `blockIdx.y` — 블록 인덱스

```cuda
vectorAdd(/* 인자 */) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    c[idx] = a[idx] + b[idx];
}
```

16개 벡터 요소를 4개 블록(블록당 4개 요소)으로 나누면:
- 각 스레드는 블록 내에서 고유 ID(0~3)를 가진다.
- 각 블록도 고유 ID(0~3)를 가진다.
- `idx = blockIdx.x * blockDim.x + threadIdx.x` 공식으로 전역 인덱스를 계산한다.
- 인덱스는 2D 또는 3D가 될 수 있어 이미지나 3D 객체의 물리적 크기에 대응 가능하다.

### 공유 메모리

공유 메모리는 온칩 스크래치 패드 메모리로, `__shared__`로 선언한다. 오프칩에 위치하는 전역 메모리보다 **접근이 훨씬 빠르다**. 공유 메모리는 **하나의 CUDA 블록 내에서만** 접근 가능하다.

```cuda
__global__ void vectorAdd(int* a, int* b, int* c) {
    __shared__ int sharedMemory[N];
    int idx = threadIdx.x;

    // 전역 메모리에서 공유 메모리로 로드
    sharedMemory[idx] = a[idx] + b[idx];

    // 블록 내 모든 스레드가 로딩을 완료할 때까지 대기
    __syncthreads();

    // 공유 메모리 데이터를 사용하여 연산
    c[idx] = sharedMemory[idx];
}
```

### 실행 순서

스레드와 블록 모두 **실행 순서가 보장되지 않는다**:
- 스레드: 4, 3, 2, 1 또는 1, 3, 4, 2 또는 1, 2, 3, 4 등 어떤 순서도 가능
- 블록: 오름차순, 내림차순, 또는 여러 블록이 동시에 실행될 수 있음

### 스레드 동기화

```cuda
__global__ void Kernel() {
    // work-1
    __syncthreads();  // 동기화
    // work-2
}
```

스레드가 임의 순서로 실행되고 서로 다른 속도로 진행하므로, `__syncthreads()`로 지정된 코드 지점까지 모든 스레드를 동기화한다. **블록 간 동기화는 별도의 커널 실행**으로 수행한다.

### 일반적인 동기화 패턴

```cuda
__global__ void Kernel() {
    // 1단계: 공유 메모리에 로드
    __syncthreads();
    // 2단계: 연산
    __syncthreads();
    // 3단계: 저장
}
```

모든 스레드가 독립적으로 작업하고 동기화 지점에서 만난다. 이 패턴은 **BSP(Bulk Synchronous Parallel)** 모델이라 부른다. 데이터 의존성과 예측 불가능한 실행 순서 때문에 스레드 동기화가 필수적이다.

### 커널 실행

호스트 코드에서 삼중 꺾쇠로 커널을 실행하며, 블록 수와 스레드 수를 지정한다. 그리드의 총 스레드 수 = 블록 수 × 블록당 스레드 수.

```cuda
int main() {
    sortKernel<<<1, 1>>>(d_data, dataSize);             // 1 블록 × 1 스레드
    addKernel<<<gridDim, blockDim>>>(d_data, d_data,
                                     d_result, dataSize); // gridDim × blockDim 스레드
    storeKernel<<<1, 1>>>(d_result, dataSize);           // 1 블록 × 1 스레드
}
```

여러 커널이 서로 다른 작업을 서로 다른 그리드/블록 구성으로 처리할 수 있다.

### 메모리 공간과 블록/스레드

| 메모리 유형 | 범위 | 특징 |
|------------|------|------|
| **공유 메모리(Shared)** | CUDA 블록 내 | 온칩, 빠른 접근 |
| **전역 메모리(Global)** | 커널 내 모든 블록 | 오프칩, 모든 블록이 접근 가능 |
| **로컬 메모리(Local)** | CUDA 스레드 내 | 스레드별 저장 공간 |
| **상수 메모리(Constant)** | 읽기 전용 | 용량 작음, 소규모 데이터 |
| **텍스처 메모리(Texture)** | 읽기 전용 | 2D 접근에 최적화된 구조 |

**핵심 제약:** 실행 계층에 따라 정보 공유가 제한된다. 공유 메모리 데이터는 하나의 CUDA 블록 내에서만 보이며, 이는 하나의 SM 내에 국한된다는 뜻이다. 한 블록의 공유 메모리 데이터를 다른 블록에서 사용하려면 명시적 통신이 필요하다.

---

## Lesson 2: 점유율(Occupancy)

### 하나의 SM에 몇 개의 CUDA 블록이 실행되는가?

하나의 SM에 하나의 블록만 실행되는 것이 아니다. 여러 블록이 하나의 SM에 공존할 수 있지만, 그 수는 하드웨어 제약에 따라 달라진다.

### 점유율

SM당 CUDA 블록 수는 다음 세 가지에 따라 결정된다:
- **레지스터 수**
- **공유 메모리**
- **스레드 수**

정확한 하드웨어 설정은 GPU 마이크로 아키텍처와 세대에 따라 다르다.

### 점유율 계산 예제

SM 용량: 256 스레드, 64 KB 레지스터, 32 KB 공유 메모리
블록 요구: 32 스레드, 2 KB 공유 메모리, 스레드당 64 레지스터

| 제약 조건 | 계산 | 결과 |
|----------|------|------|
| 스레드 | 256 / 32 | **8 블록** |
| 레지스터 | (64 × 1,024) / (64 × 32) | 32 블록 |
| 공유 메모리 | 32,000 / 2,000 | 16 블록 |
| **최종 답 (최솟값)** | | **8 블록** |

### 블록당 스레드 수 결정 요인

- 호스트가 `kernelName<<<numBlocks, threadsPerBlock>>>(args)`로 스레드 수를 지정한다.
- 커널 실행 시점에 결정된다.
- 컴파일러가 블록당 필요한 레지스터 수를 결정한다 (마이크로 아키텍처에 따라 다름).
- CPU와 달리 CUDA의 레지스터 수는 가변적이다.
- 공유 메모리 크기는 코드에 의해 결정된다 (예: `__shared__ int sharedMemory[1000]` = 4,000 바이트).
- 이 모든 요인의 조합으로 점유율이 결정된다.

### 점유율이 중요한 이유

- 높은 점유율 = 더 많은 병렬성
- 더 많은 하드웨어 자원 활용은 일반적으로 더 나은 성능으로 이어진다
- 예외 사례는 이후 강의에서 다룬다

**비교:**
- 사례 A: 4 SM × 1 블록 × 6 스레드 = 총 24 스레드
- 사례 B: 4 SM × 2 블록 × 4 스레드 = 총 32 스레드 (더 높은 점유율)

---

## Lesson 3: 호스트 코드

### 전역 메모리 관리

공유 메모리는 CUDA 블록 내에서만 보인다. 전역 메모리는 모든 블록에서 접근 가능하지만, **디바이스 메모리에 위치**하여 I/O 인터페이스 분리로 인해 CPU 측에서는 보이지 않는다. 호스트에서 디바이스 메모리를 관리하려면 명시적 API가 필요하다.

**두 가지 핵심 API:**

| API | 역할 | 예시 |
|-----|------|------|
| `cudaMalloc` | GPU 메모리 할당 | `cudaMalloc(&d_A, size)` |
| `cudaMemcpy` | CPU ↔ GPU 데이터 전송 | `cudaMemcpyHostToDevice` / `cudaMemcpyDeviceToHost` |

이후 강의에서 명시적 데이터 복사 없이 사용할 수 있는 **통합 메모리(Unified Memory)** 를 다룬다.

### 벡터 덧셈 전체 흐름

1. **호스트**: 메모리 할당, 데이터 전송
2. **커널**: GPU 스레드에서 병렬 실행
3. **호스트**: 결과 회수, 정리

---

## Lesson 4: CUDA 스텐실 연산

### 스텐실 연산 복습

이웃 요소를 사용하여 값을 계산하는 연산이다. 예를 들어 동·서·남·북 4개 이웃의 평균으로 C를 계산한다.

**병렬화 접근:**
- 서로 다른 CUDA 블록이 서로 다른 요소를 계산한다.
- 각 스레드가 하나의 스텐실 연산 요소를 담당한다.
- 요소가 여러 번 재사용되므로 **공유 메모리에 적합**하다.

### 스레드별 연산: 3단계 패턴

1. **1단계**: 전역 메모리에서 공유 메모리(온칩)로 데이터 로드
2. **2단계**: 스텐실 연산 수행
3. **3단계**: 결과를 전역 메모리에 기록

1단계와 2단계 사이에 `__syncthreads()`로 동기화하여 공유 메모리가 완전히 로드된 후 연산을 시작한다.

### 경계 처리

공유 메모리의 블록 범위 제한으로 경계 문제가 발생한다. 경계의 값을 계산하려면 **인접 블록 영역의 데이터**가 필요하다.

**해결 방법:** 각 공유 메모리가 핵심 데이터와 함께 **경계 요소도 로드**한다. MPI의 명시적 경계 통신과 유사하지만, 단순히 경계 데이터를 공유 메모리로 가져오는 방식이다 (읽기 전용 접근).

```cuda
// 공유 메모리에 데이터 로드
__shared__ float sharedInput[sharedDim][sharedDim];
int sharedX = threadIdx.x + filterSize / 2;
int sharedY = threadIdx.y + filterSize / 2;
```

**경계 조건 처리:**

```cuda
if (x >= 0 && x < width && y >= 0 && y < height) {
    sharedInput[sharedY][sharedX] = input[y * width + x];
} else {
    sharedInput[sharedY][sharedX] = 0.0f;  // 경계 조건 처리
}
```

**내부 요소에서만 연산 수행:**

```cuda
for (int i = 0; i < filterSize; i++) {
    for (int j = 0; j < filterSize; j++) {
        result += sharedInput[threadIdx.y + i][threadIdx.x + j]
                * filter[i][j];
    }
}
```

실제 연산 영역보다 더 많은 스레드를 생성하고, 내부 스레드만 연산을 수행하는 방식으로 if-else 문을 최소화한다.

---

## 이 챕터에서 기억할 것

1. **CUDA 실행 계층** — 그리드 → 블록 → 스레드. 블록은 SM에, 스레드는 SP에 매핑된다.
2. **`idx = blockIdx.x * blockDim.x + threadIdx.x`** — 전역 인덱스 계산의 기본 공식이다.
3. **공유 메모리**는 온칩에 위치하여 전역 메모리보다 훨씬 빠르지만, **블록 내에서만** 접근 가능하다.
4. **`__syncthreads()`** — 블록 내 스레드 동기화. 블록 간 동기화는 별도 커널 실행으로 수행한다.
5. **점유율(Occupancy)** — SM당 블록 수는 스레드 수, 레지스터 수, 공유 메모리의 **최솟값**으로 결정된다.
6. **스텐실 연산**은 공유 메모리 + 경계 로딩 + 동기화의 전형적인 사용 패턴이다.

> 이 문서는 Georgia Tech CS8803 GPU Hardware and Software — Module 3: GPU Programming Introduction을 한글로 요약한 것입니다.
> 세부 사항은 [원본 강의 노트](https://lowyx.com/posts/gt-gpu-notes/#module-3-gpu-programming-introduction)를 참조한다.
