---
title: Parallel Programming (병렬 프로그래밍) — 한글 정리
description: Georgia Tech CS8803 Module 2 — 병렬 프로그래밍 패턴, OpenMP, MPI, 공유/분산 메모리 프로그래밍
---

> **원문:** [CS8803 OMSCS — GPU Hardware and Software Notes (Module 2)](https://lowyx.com/posts/gt-gpu-notes/#module-2-parallel-programming)

## 학습 목표

- 다양한 병렬 프로그래밍 패러다임을 개관한다
- 공유 메모리 프로그래밍의 기초를 설명한다
- OpenMP와 MPI의 핵심 구성 요소를 이해한다

---

## Lesson 1: 병렬 프로그래밍 패턴

### 병렬 애플리케이션 작성 방법

원래 프로그램에서 태스크를 식별하고, 작업을 분할하며, 데이터를 재구성하여 여러 프로세서에서 병렬 실행을 가능하게 하는 것이 기본 접근 방식이다.

### 병렬 프로그래밍 작성 단계

1. **동시성 발견(Discover concurrency)** — 문제 내에서 병렬화 기회를 식별한다.
2. **알고리즘 구조화(Structure the algorithm)** — 식별한 병렬성을 효과적으로 활용하도록 코드를 조직한다.
3. **구현(Implementation)** — 적절한 프로그래밍 언어와 도구를 선택한다.
4. **실행 및 최적화(Execution and optimization)** — 병렬 시스템에서 실행하고 성능을 미세 조정한다.

### 다섯 가지 병렬 프로그래밍 패턴

1. **마스터/워커(Master/Worker) 패턴** — 하나의 마스터가 워커 스레드 풀을 관리하며, 공유 큐에서 태스크를 꺼내 실행한다. 서로 다른 워커가 서로 다른 태스크를 동시에 처리한다. 작업을 더 작은 독립 조각으로 분할할 때 유용하다.

2. **루프 병렬화(Loop Parallelism) 패턴** — 반복적이고 독립적인 루프 반복을 병렬로 실행한다. 각 반복이 서로 다른 데이터에 대해 동일한 작업을 수행하므로 병렬화에 적합하다. 예: `for (i = 0; i < 16; i++) c[i] = A[i] + B[i];`

3. **SPMD(Single Program, Multiple Data) 패턴** — 모든 처리 요소가 동일한 프로그램을 병렬로 실행하되, 각각 서로 다른 데이터에 대해 동작한다. GPU 프로그래밍에서 가장 널리 사용된다.

4. **포크/조인(Fork/Join) 패턴** — 부모 태스크가 새 태스크를 생성(fork)한 뒤, 완료를 대기(join)하고 나서 계속 진행한다. 직렬과 병렬 처리를 결합하며, 단일 진입점 프로그래밍이 특징이다.

5. **파이프라인(Pipeline) 패턴** — CPU 파이프라인 구조와 유사하며, 병렬 프로세서들이 서로 다른 작업 단계를 처리한다. 신호 처리, 그래픽스 파이프라인, 압축 워크플로 등 데이터 스트림 처리에 적합하다.

---

## Lesson 2: OpenMP vs MPI (Part 1)

### 공유 메모리 프로그래밍

공유 메모리 시스템에서는 모든 프로세서가 동일한 메모리에 접근한다. 한 프로세서가 메모리를 갱신하면, 동일한 위치를 읽는 다른 프로세서에게 즉시 변경 사항이 보인다.

### 분산 메모리 프로그래밍

공유 메모리와 달리, 분산 메모리 시스템에서는 각 프로세서가 자체 메모리 공간을 가진다. 다른 프로세서의 데이터에 접근하려면 명시적인 **메시지 전달(message passing)** 이 필요하다.

### OpenMP 개요

OpenMP는 공유 메모리 병렬 프로그래밍을 위한 개방형 표준이다. 컴파일러 지시문, 라이브러리 루틴, 환경 변수로 구성되어 병렬 프로그램을 작성할 수 있게 한다.

### 병렬 프로그래밍의 핵심 작업

- 병렬화(Parallelization)
- 스레드 수 지정(Thread count specification)
- 스케줄링 전략(Scheduling strategies)
- 데이터 공유 정책(Data sharing policies)
- 동기화 메커니즘(Synchronization mechanisms)

### 스레드(Thread)란?

스레드는 명령어 시퀀스를 실행하며, 자체 레지스터와 스택 메모리를 유지한다. 스레드는 하드웨어가 아닌 **소프트웨어 개념**으로, 하나의 코어가 여러 스레드를 실행할 수 있다. CPU와 GPU에서 스레드의 의미는 다르다.

### 데이터 분해 복습

데이터 분해를 사용할 때, 배열을 코어별로 분할한다. 절반은 코어 1, 나머지 절반은 코어 2에 할당하여 병렬 실행한다. 현대 CPU는 하나의 코어에서 여러 스레드를 실행할 수 있지만, 예제에서는 단순화를 위해 코어당 하나의 스레드를 가정한다.

### 벡터 합 예제

```c
int main() {
  const int size = 10;
  int data[size];
  int sum = 0;
  for (int i = 0; i < size; ++i) {
    sum += data[i];
  }
  std::cout << "Sum of array elements: " << sum << std::endl;
  return 0;
}
```

### 수동 병렬화 과정

1. 두 개의 스레드를 생성한다.
2. 배열을 절반으로 나누어 각 스레드에 할당한다.
3. 리덕션(reduction) 연산으로 부분합을 병합한다.

### 공유 메모리에서의 리덕션 연산

각 스레드가 부분합을 계산한다. 한 스레드가 메모리의 총합을 갱신하면, 다른 스레드가 갱신된 값을 읽어 자신의 부분합을 더하고 총합을 갱신한다. **뮤텍스(Mutex)** 가 공유 값에 대한 동시 갱신을 방지한다.

### 뮤텍스(Mutex)

상호 배제(Mutual Exclusion)를 보장하여 하나의 스레드만 임계 코드 구간에 동시 접근하도록 한다.

- **Lock**: 임계 구간 진입 전 뮤텍스를 획득한다.
- **Unlock**: 완료 후 뮤텍스를 해제하여 다른 스레드가 접근하게 한다.

이는 공유 변수 갱신 시 **데이터 경합(data race)** 을 방지한다.

### 저수준 프로그래밍 vs OpenMP

저수준 p-thread 프로그래밍은 스레드 생성, 조인, 뮤텍스 연산을 수동으로 처리해야 한다. OpenMP는 이를 크게 단순화한다:

```c
#include <iostream>
#include <omp.h>

int main() {
  const int size = 1000;
  int data[size];
  int sum = 0;

  #pragma omp parallel for reduction(+:sum)
  for (int i = 0; i < size; ++i) {
    sum += data[i];
  }

  std::cout << "Sum of array elements: " << sum << std::endl;
  return 0;
}
```

`#pragma omp parallel for reduction(+:sum)` 한 줄을 추가하면 루프 병렬화 패턴이 자동으로 스레드를 생성·관리한다. C/C++ 및 Fortran에서 동작하며, 컴파일러가 지시문을 런타임 라이브러리 호출로 치환하며, 라이브러리 함수가 스레드 생성과 조인을 처리한다. 일반 문법은 `#pragma omp directive [clause [clause] ...]` 형식이다. 지시문(directive)은 주요 OpenMP 구성 요소이고, 절(clause)은 `reduction(+:sum)` 같은 추가 정보를 제공한다.

---

## Lesson 3: OpenMP vs MPI (Part 2)

### 스레드 수 제어

`OMP_NUM_THREADS` 환경 변수 또는 `omp_set_num_threads()` 함수로 스레드 수를 제어한다. 기본값은 하드웨어 병렬성(코어 수)과 같다.

### 스케줄링

백만 개 요소를 5개 스레드, 2개 코어에 분배할 때 문제가 발생한다. 3개 스레드를 가진 코어는 2개 스레드가 균등 분배된 경우보다 자원이 부족하다.

| 방식 | 설명 | 특징 |
|------|------|------|
| **정적(Static)** | 각 스레드에 일정한 작업 청크를 할당 (예: 200k 요소) | 모든 스레드가 대략 균등하게 진행된다고 가정 |
| **동적(Dynamic)** | 최소 작업(1 요소)을 할당, 완료 시 추가 요청 | 효율적이지만 오버헤드가 큼 |
| **동적 + 청크** | 더 큰 초기 청크(1,000 요소) 할당 | 오버헤드를 줄이면서 유연성 유지 |
| **가이드(Guided)** | 큰 청크(1,000 요소)로 시작, 점진적으로 감소 | 빠른 스레드는 큰 청크, 느린 스레드는 작은 청크를 받음 |

스케줄링 선택은 워크로드 특성과 스레드 진행 역학에 따라 달라진다.

### 데이터 공유

데이터를 **사적(private)** 데이터(부분합 등)와 **공유(shared)** 데이터(전체 합)로 구분한다. 프로그래머가 데이터 공유 정책을 명시적으로 지정해야 한다.

### 스레드 동기화

올바른 실행을 위해 필수적이며, 배리어, 임계 구간, 원자적 연산이 있다.

**배리어(Barrier):**

```c
#pragma omp barrier
```

모든 참여 스레드가 특정 지점에 도달할 때까지 대기한 후 진행한다. 태스크 간 의존성(예: 정렬 후 갱신)이 있을 때 중요하다.

**임계 구간(Critical Section):**

```c
#pragma omp parallel num_threads(4)
{
  #pragma omp critical
  {
    // 한 번에 하나의 스레드만 실행
  }
}
```

4개 스레드가 있어도 하나만 임계 구간에 진입하여 데이터 경합을 방지한다.

**원자적 연산(Atomic):**

```c
#pragma omp parallel for
for (int i = 0; i < num_iterations; ++i) {
  #pragma omp atomic
  counter++;
}
```

카운터 증가는 값 로드 → 1 더하기 → 결과 저장의 3단계가 필요하다. 원자적 연산은 이 3단계가 모두 수행되거나 전혀 수행되지 않음을 보장한다. 뮤텍스 또는 하드웨어 지원으로 구현된다.

### 병렬 섹션

루프 외의 병렬 작업에 사용한다:

```c
#pragma omp parallel sections
{
  #pragma omp section
  { /* work-1 */ }
  #pragma omp section
  { /* work-2 */ }
}
```

work-1과 work-2가 병렬로 실행된다.

**ordered 구문**: 스레드가 오름차순으로 실행되도록 보장한다.

**single 구문**: 정확히 하나의 스레드만 해당 구간을 실행한다. 초기화 작업에 유용하다.

---

## Lesson 4: MPI 프로그래밍

### OpenMP와 MPI를 함께 배우는 이유

CUDA 프로그래밍은 공유 메모리와 분산 메모리 요소를 모두 결합한다. 일부 메모리 영역은 코어 간에 공유되고, 다른 영역은 명시적 통신이 필요하므로, 두 패러다임을 모두 이해하는 것이 필요한 기반이 된다.

### MPI 프로그래밍

MPI(Message Passing Interface)는 프로세스 간 데이터 교환을 가능하게 한다.

- `MPI_send()` — 한 프로세스에서 값을 전송한다.
- `MPI_recv()` — 다른 프로세스에서 값을 수신한다.

### 브로드캐스트

`MPI_Bcast()`는 한 프로세스에서 다른 모든 프로세스로 데이터를 집단적(collective)으로 브로드캐스트하여 전역 정보를 공유한다.

### 스텐실 연산

고성능 컴퓨팅에서 흔히 사용되며, 이웃 데이터를 사용하여 결과를 계산한다. 예를 들어 값 C를 계산하려면 4개 이웃의 평균을 구한다. 이 연산을 전체 데이터셋에 적용한다.

### 경계 정보 통신

분산 메모리 시스템에서 스텐실 연산을 분배하면, 각 프로세스는 자신의 메모리 영역에만 접근할 수 있다. 여러 메모리 영역에 걸친 데이터 의존성을 해결하려면 **경계 정보를 명시적 메시지로 교환**해야 한다. 이것은 MPI의 핵심 개념으로, 인접 데이터 영역을 관리하는 프로세서 간 정보 교환을 처리한다.

---

## 이 챕터에서 기억할 것

1. **병렬 프로그래밍 4단계** — 동시성 발견 → 알고리즘 구조화 → 구현 → 실행 및 최적화.
2. **SPMD 패턴**이 GPU 프로그래밍의 핵심이다 — 같은 프로그램, 서로 다른 데이터.
3. **공유 메모리(OpenMP) vs 분산 메모리(MPI)** — CUDA는 두 요소를 모두 결합한다.
4. **동기화 메커니즘** — 뮤텍스, 배리어, 임계 구간, 원자적 연산으로 데이터 경합을 방지한다.
5. **스케줄링 전략** — 정적/동적/가이드 방식이 있으며, 워크로드 특성에 따라 선택한다.

> 이 문서는 Georgia Tech CS8803 GPU Hardware and Software — Module 2: Parallel Programming을 한글로 요약한 것입니다.
> 세부 사항은 [원본 강의 노트](https://lowyx.com/posts/gt-gpu-notes/#module-2-parallel-programming)를 참조한다.
