---
title: "Programming Model (프로그래밍 모델) — 한글 요약"
sidebar:
  label: "02. Programming Model"
---

> 원본: https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html

이 섹션은 CUDA의 프로그래밍 모델을 높은 수준에서 소개한다. 특정 프로그래밍 언어에 독립적인 개념으로, 모든 CUDA 지원 언어에 적용된다.

---

## 1.2.1 이기종 시스템 (Heterogeneous Systems)

CUDA는 GPU와 CPU를 모두 포함하는 컴퓨팅 환경을 전제한다.

- **호스트(Host)**: CPU와 CPU에 직접 연결된 메모리
- **디바이스(Device)**: GPU와 GPU에 직접 연결된 메모리

SoC(System-on-Chip) 아키텍처에서는 하나의 패키지를 공유할 수 있고, 대규모 시스템에서는 여러 CPU와 GPU를 포함할 수 있다.

CUDA 애플리케이션은 일부 코드를 GPU에서 실행하지만, **항상 CPU에서 시작**한다. 호스트 코드는 메모리 공간 간 데이터 전송을 관리하고, GPU 실행을 시작하며, 완료를 모니터링한다. 두 프로세서가 동시에 동작할 수 있어 최적 성능을 얻는다.

**핵심 용어:**
- **디바이스 코드(Device code)**: GPU에서 실행되는 코드
- **커널(Kernel)**: GPU 함수 (역사적 명명 규칙)
- **런칭(Launching)**: 커널 실행을 시작하는 것

> GPU 스레드는 CPU 스레드와 유사하지만 정확성과 성능 면에서 중요한 차이점이 있다.

---

## 1.2.2 GPU 하드웨어 모델 (GPU Hardware Model)

GPU는 개념적으로 다음 구성 요소로 이루어진다:

### 구성 요소

- **스트리밍 멀티프로세서(Streaming Multiprocessors, SM)**: 그래픽 프로세싱 클러스터(GPC)로 조직됨
- 각 SM 포함: 로컬 레지스터 파일, 통합 데이터 캐시(shared memory + L1 cache), 연산 기능 유닛
- 런타임에서 통합 캐시의 L1과 공유 메모리 비율을 조정할 수 있다

```
┌─────────────────────────────────────────────┐
│                    GPU                       │
│  ┌─────────────────────────────────────┐    │
│  │            GPC (Graphics            │    │
│  │        Processing Cluster)          │    │
│  │  ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐  │    │
│  │  │ SM  │ │ SM  │ │ SM  │ │ SM  │  │    │
│  │  │     │ │     │ │     │ │     │  │    │
│  │  │Regs │ │Regs │ │Regs │ │Regs │  │    │
│  │  │L1/SM│ │L1/SM│ │L1/SM│ │L1/SM│  │    │
│  │  └─────┘ └─────┘ └─────┘ └─────┘  │    │
│  └─────────────────────────────────────┘    │
│              L2 Cache (공유)                  │
│          ┌───────────────────┐               │
│          │  Global Memory    │               │
│          └───────────────────┘               │
└─────────────────────────────────────────────┘
```

### 1.2.2.1 스레드 블록과 그리드 (Thread Blocks and Grids)

커널 런칭은 수백만 개의 스레드로 작업을 분배하며, 계층적으로 조직된다:

- 스레드 → **스레드 블록(Thread Block)** 으로 그룹화
- 스레드 블록 → **그리드(Grid)** 로 조직
- 블록과 그리드는 1D, 2D, 3D 레이아웃 지원 → 데이터 매핑이 직관적

**빌트인 변수**로 각 스레드가 자신의 위치를 결정:
- 스레드 블록 내 위치
- 그리드 내 블록 위치
- 블록/그리드 차원

**스케줄링 모델:**
- 하나의 스레드 블록의 모든 스레드는 **단일 SM에서 실행**됨 → 효율적 통신/동기화
- 스레드 블록은 온칩 공유 메모리를 사용한 스레드 간 데이터 교환 가능
- **핵심 제약**: 스레드 블록 간 스케줄링 순서는 보장되지 않음. 한 블록이 다른 블록의 결과에 의존할 수 없음

> **설계 원칙**: 이 모델은 블록 간 데이터 의존성을 요구하지 않아, 어떤 크기의 GPU에서도 임의의 큰 그리드를 실행할 수 있다. 스레드 블록은 병렬 또는 순차적으로 실행 가능하며, 이는 확장성의 핵심이다.

#### 1.2.2.1.1 스레드 블록 클러스터 (Thread Block Clusters)

Compute Capability 9.0+ GPU에서 지원하는 선택적 기능:

- 인접 스레드 블록의 그룹으로, 1D/2D/3D 레이아웃
- 같은 GPC 내에서 동시 실행
- **분산 공유 메모리(Distributed Shared Memory)**: 클러스터 내 모든 블록의 공유 메모리에 접근 가능

### 1.2.2.2 워프와 SIMT (Warps and SIMT)

스레드 블록 내에서 스레드는 **32개씩 그룹**을 이루어 **워프(Warp)** 가 된다.

**SIMT(Single-Instruction Multiple-Threads) 실행 모델:**
- 워프의 모든 스레드가 동일한 커널 코드를 동시에 실행
- 개별 스레드는 다른 코드 분기를 따를 수 있음
- 워프 레인(lane) 번호: 0-31

**워프 발산(Warp Divergence):**
- 조건문에서 일부 스레드만 `true`이면, 나머지는 **마스킹(masking off)** 됨
- 모든 스레드가 동일한 제어 흐름을 따를 때 GPU 활용도가 극대화됨

```
워프 (32 스레드): [T0][T1][T2][T3]...[T31]

if (threadIdx.x % 2 == 0) {
    // T0, T2, T4, ... 만 실행 (짝수)
    // T1, T3, T5, ... 는 마스킹 (대기)
}
```

> **최적화 팁**: 스레드 블록의 스레드 수는 **32의 배수**가 되어야 한다. 그렇지 않으면 마지막 워프에서 사용되지 않는 레인이 낭비된다.

**SIMT vs SIMD:**
- SIMD: 고정된 단일 제어 흐름, 고정 데이터 폭
- SIMT: 스레드별 제어 흐름 허용, 고정 데이터 폭 제약 없음

---

## 1.2.3 GPU 메모리 (GPU Memory)

효율적인 메모리 활용은 연산 성능만큼 중요하다.

### 1.2.3.1 이기종 시스템의 DRAM 메모리

```
┌──────────┐           ┌──────────┐
│   CPU    │   PCIe/   │   GPU    │
│          │  NVLINK   │          │
│ System   │◄─────────►│ Global   │
│ Memory   │           │ Memory   │
│ (Host)   │           │ (Device) │
└──────────┘           └──────────┘
```

- GPU와 CPU는 각각 직접 연결된 DRAM을 가짐
- GPU DRAM = **글로벌 메모리(Global Memory)** (모든 SM에서 접근 가능)
- CPU DRAM = **시스템 메모리** 또는 **호스트 메모리**
- 가상 메모리 주소 공간은 통합(unified)되어 있음
- CUDA는 GPU 메모리 할당, CPU-GPU 간 복사 등의 API 제공

### 1.2.3.2 GPU의 온칩 메모리 (On-Chip Memory)

| 메모리 유형 | 범위 | 용도 |
|------------|------|------|
| **레지스터 파일(Register File)** | 스레드 단위 | 스레드 로컬 변수 저장 (컴파일러 할당) |
| **공유 메모리(Shared Memory)** | 블록 단위 | 블록 내 스레드 간 데이터 교환 |
| **L1 캐시** | SM 단위 | 통합 데이터 캐시의 일부 |
| **L2 캐시** | GPU 전체 | 모든 SM이 공유 |
| **상수 캐시(Constant Cache)** | SM 단위 | 상수 메모리 접근 최적화 |

**리소스 제약:**
- 레지스터 파일과 통합 캐시는 유한한 크기
- 스레드 블록을 SM에 스케줄링하려면: `(스레드당 레지스터 × 스레드 수) ≤ SM 사용 가능 레지스터`
- 초과 시 커널 런칭 불가 → 스레드 수를 줄여야 함

> **할당 단위**: 레지스터는 **스레드 단위**, 공유 메모리는 **블록 단위**로 할당된다.

### 1.2.3.3 유니파이드 메모리 (Unified Memory)

일반적으로:
- CPU 할당 메모리 → CPU 코드만 접근 가능
- GPU 할당 메모리 → GPU 커널만 접근 가능

**유니파이드 메모리**는 이 제한을 해결:
- CPU와 GPU 모두에서 접근 가능한 할당
- CUDA 런타임 또는 하드웨어가 필요 시 자동으로 데이터를 올바른 위치로 이동

> **성능 주의**: 유니파이드 메모리는 프로그래밍을 단순화하지만, **메모리 마이그레이션을 최소화하고 데이터가 상주하는 프로세서에서 직접 접근하는 것이 최적 성능을 달성**하는 방법이다.

---

## 이 챕터에서 기억할 것

1. **CUDA는 이기종 컴퓨팅** — 항상 CPU에서 시작, GPU는 병렬 연산을 가속
2. **스레드 계층**: 스레드 → 워프(32개) → 스레드 블록 → 그리드
3. **블록 간 독립성** — 스레드 블록은 서로의 결과에 의존할 수 없음 (확장성 보장)
4. **SIMT 모델** — 워프 내 32개 스레드가 동시에 같은 명령어를 실행, 분기 시 마스킹 발생
5. **메모리 계층**: 레지스터(스레드) → 공유 메모리(블록) → L1(SM) → L2(GPU) → 글로벌 메모리(DRAM)
6. **유니파이드 메모리** — CPU/GPU 모두 접근 가능하지만, 데이터 로컬리티가 성능의 핵심

---

*이 문서는 NVIDIA CUDA Programming Guide v13.1의 Programming Model 챕터를 한글로 요약한 것입니다.*
*원본: https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html*
